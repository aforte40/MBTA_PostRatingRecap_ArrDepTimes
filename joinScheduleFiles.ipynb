{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "colTypes = {\n",
    "    'route_id': 'string',\n",
    "    'direction_id': 'category',\n",
    "    'half_trip_id': 'string',\n",
    "    'stop_id': pd.Int32Dtype(),\n",
    "    'time_point_order': pd.Int8Dtype(),\n",
    "    'point_type': 'category', \n",
    "    'standard_type': 'category'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antonio.forte\\AppData\\Local\\Temp\\ipykernel_10528\\3995012852.py:10: DtypeWarning: Columns (0,3,5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(os.path.join(txt_path, f), sep=',') for f in files]\n",
      "C:\\Users\\antonio.forte\\AppData\\Local\\Temp\\ipykernel_10528\\3995012852.py:10: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(os.path.join(txt_path, f), sep=',') for f in files]\n"
     ]
    }
   ],
   "source": [
    "# Import all *.txt files in the gtfsSchedule folder and parse them as dataframes\n",
    "# add the gtfsSchedule folder and subfolders to the current path\n",
    "txt_path = ('gtfsSchedule\\\\gtfs_2022-12-18_2023-03-11_Winter2023PostRecap')\n",
    "# List of dataframe names: remove the '.txt' extension from the filenames\n",
    "df_names = [filename[:-4] for filename in os.listdir(txt_path)] \n",
    "# Read txt files into dataframes and assign them the names in df_names\n",
    "# create a list of filenames\n",
    "files = os.listdir(txt_path)\n",
    "# create a list of dataframes\n",
    "dfs = [pd.read_csv(os.path.join(txt_path, f), sep=',') for f in files]\n",
    "# create a dictionary of dataframes\n",
    "gtfsSchedule = dict(zip(df_names, dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df starting from gtfsScehdule['trips] where trip_ids are matched to service_ids\n",
    "# This will be the base df for the analysis\n",
    "trips = gtfsSchedule['trips']\n",
    "calendar = gtfsSchedule['calendar']\n",
    "calendar_attributes = gtfsSchedule['calendar_attributes']\n",
    "calendar_dates = gtfsSchedule['calendar_dates']\n",
    "# Assign to every row in trips the corresponding rating_start_date and rating_end_date contained in calendar_attributes matching the two datasets by service_id\n",
    "# Add the service_id field from the calendar dataframe to the trips dataframe, without including the other fields\n",
    "trips = pd.merge(trips, calendar_attributes[['service_id', 'rating_start_date', 'rating_end_date']], on='service_id')\n",
    "\n",
    "# Extract the stop_times df from the dict\n",
    "stop_times = gtfsSchedule['stop_times']\n",
    "# Make sure trip_id column from both the df has the same dtype not to miss any correspondence\n",
    "trips['trip_id'] = trips['trip_id'].astype(str)\n",
    "stop_times['trip_id'] = stop_times['trip_id'].astype(str)\n",
    "\n",
    "# Merge stop_times with trips\n",
    "schedule = pd.merge(stop_times, trips, on='trip_id', how='left')\n",
    "\n",
    "# Convert date values to datetime objects\n",
    "schedule['rating_start_date'] = pd.to_datetime(schedule['rating_start_date'], format='%Y%m%d')\n",
    "schedule['rating_end_date'] = pd.to_datetime(schedule['rating_end_date'], format='%Y%m%d')\n",
    "# Extract only records whose route_id is an integer, i.e., bus routes\n",
    "schedule = schedule[schedule.route_id.str.isnumeric()]\n",
    "# Drop the columns that are not needed for the analysis\n",
    "drop_colums = ['trip_headsign', 'trip_short_name',\n",
    "        'shape_id', 'wheelchair_accessible',\n",
    "       'trip_route_type', 'route_pattern_id', 'bikes_allowed', 'stop_headsign',\n",
    "       'pickup_type', 'drop_off_type', \n",
    "       'continuous_pickup', 'continuous_drop_off']\n",
    "schedule = schedule.drop(columns=drop_colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.loc[schedule.route_id=='1'].to_csv('test_route1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files with arrival and departure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv files from 2022 and 2023, cast them into a single dataframe, and filter out the bus routes included within the dates of the scheduled df\n",
    "# Import the csv files\n",
    "csv_path = 'MBTA_ArrivalDepartureTimes'\n",
    "foldername = 'MBTA_Bus_Arrival_Departure_Times'\n",
    "\n",
    "adt2022_list = []\n",
    "adt2023_list = []\n",
    "import_process = 0\n",
    "if import_process:\n",
    "    # Arrival/Departure times 2022\n",
    "    adt_2022 = os.path.join(csv_path, (foldername + '_' + '2022'))\n",
    "    csv2022_files = os.listdir(adt_2022)\n",
    "    # Arrival/Departure times 2023\n",
    "    adt_2023 = os.path.join(csv_path, (foldername + '_' + '2023'))\n",
    "    csv2023_files = os.listdir(adt_2023)\n",
    "    \n",
    "    for i in range(12):\n",
    "        print(i)\n",
    "        adt2022_list.append(pd.read_csv(os.path.join(adt_2022, csv2022_files[i]), sep=','))\n",
    "        adt2023_list.append(pd.read_csv(os.path.join(adt_2023, csv2023_files[i]), sep=','))\n",
    "\n",
    "    # Build a single dataframe\n",
    "    adt_df = pd.concat((pd.concat(adt2023_list, axis = 0), pd.concat(adt2022_list, axis = 0)), axis = 0)\n",
    "    # Keep only the rows whose service_date is within the range of the scheduled df\n",
    "    feed_info = gtfsSchedule['feed_info']\n",
    "    start_date = pd.to_datetime(feed_info.feed_start_date.values, format='%Y%m%d')\n",
    "    end_date = pd.to_datetime(feed_info.feed_end_date.values, format='%Y%m%d')\n",
    "    adt_df['service_date'] = pd.to_datetime(adt_df['service_date'], format='%Y-%m-%d')\n",
    "    adt_df = adt_df.loc[adt_df.service_date >= start_date[0]]\n",
    "    adt_df = adt_df.loc[adt_df.service_date <= end_date[0]]\n",
    "    adt_df.reset_index()\n",
    "    # Replace wrong SL3 id with the correct one\n",
    "    adt_df.loc[adt_df.route_id=='746_', 'route_id'] = '746'\n",
    "    # Use the routes gtfs file to match route_ids in the adt dataframe with their univocal identifier\n",
    "    routes = gtfsSchedule['routes']\n",
    "    adt_df = pd.merge(adt_df, routes[['route_id', 'route_short_name']], on='route_id')\n",
    "    # Change the dtype of the columns included in the colTypes to their corresponding values\n",
    "    for key, value in colTypes.items():\n",
    "        adt_df[key] = adt_df[key].astype(value)\n",
    "else:\n",
    "    # Read the file in separate chunks and concatenate them\n",
    "    chunk_size = 10**6\n",
    "    chunks = []\n",
    "    dtype_map = {\n",
    "    \"service_date\": \"string\",\n",
    "    \"route_id\": \"string\",\n",
    "    \"direction_id\": \"category\",\n",
    "    \"half_trip_id\": \"string\",\n",
    "    \"stop_id\": \"string\",\n",
    "    \"time_point_id\": \"category\", \n",
    "    \"time_point_order\": pd.Int16Dtype(),\n",
    "    \"point_type\": \"category\", \n",
    "    \"standard_type\": \"category\",  \n",
    "    \"scheduled\": \"string\",  # Consider converting to datetime later\n",
    "    \"actual\": \"string\",  # Consider converting to datetime later\n",
    "    \"scheduled_headway\": pd.Int32Dtype(),\n",
    "    \"headway\": pd.Int32Dtype(),\n",
    "    \"route_short_name\": \"category\"\n",
    "    }\n",
    "    for chunk in pd.read_csv('adt_df.csv', dtype=dtype_map, chunksize=chunk_size):\n",
    "        chunks.append(chunk)\n",
    "    adt_df = pd.concat(chunks, axis=0, ignore_index=True)\n",
    "    # Convert service_date, scheduled and actual columns to datetime objects\n",
    "    adt_df['scheduled'] = pd.to_datetime(adt_df['scheduled'], format='ISO8601')\n",
    "    adt_df['actual'] = pd.to_datetime(adt_df['actual'], format='ISO8601')\n",
    "    adt_df[\"scheduled\"] = adt_df[\"scheduled\"].dt.strftime(\"%H:%M:%S\")\n",
    "    adt_df[\"actual\"] = adt_df[\"actual\"].dt.strftime(\"%H:%M:%S\")\n",
    "    # If half_trip_id endswith '.0', trim this piece\n",
    "    adt_df['half_trip_id'] = adt_df['half_trip_id'].str.replace('.0', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = adt_df.groupby(\n",
    "    ['service_date','route_id', 'direction_id']\n",
    ")\n",
    "\n",
    "# Print first group and then break the for loop\n",
    "for name, group in groups:\n",
    "    group = group.sort_values(by='scheduled', ascending = True)\n",
    "    print(group.head())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BUS123-3-Wdy-02', 'BUS123-4-Wdy-02', 'BUS123-5-Wdy-02',\n",
       "       'BUS123-6-Wdy-02', 'ChristmasDay(Observed)-1', 'ChristmasDay-1',\n",
       "       'MartinLutherKingDay-1', 'WinterSaturday', 'WinterSunday'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtfsSchedule['trips'].loc[gtfsSchedule['trips'].route_id == '1'].service_id.unique()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
