{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_WARN_SLOW_RESOLVE_TIMEOUT'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Set the option to prevent the FutureWarning\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import datetime as dt\n",
    "\n",
    "colTypes = {\n",
    "    'route_id': 'string',\n",
    "    'direction_id': 'category',\n",
    "    'half_trip_id': 'string',\n",
    "    'stop_id': pd.Int32Dtype(),\n",
    "    'time_point_order': pd.Int8Dtype(),\n",
    "    'point_type': 'category', \n",
    "    'standard_type': 'category'\n",
    "}\n",
    "\n",
    "def split_multiple_block_id(df):\n",
    "    # 1. Extract rows with multiple block_id substrings and save their indexes\n",
    "    multiple_blocks_indexes = df[df['block_id'].str.contains(',')].index\n",
    "    df2 = df.loc[multiple_blocks_indexes].copy()\n",
    "\n",
    "    # 2. Split block_id strings into a list of strings\n",
    "    df2['block_id'] = df2['block_id'].str.split(', ')\n",
    "\n",
    "    # 3. Group by service_date, direction_id, and departure_time\n",
    "    groups = df2.groupby(['service_date', 'direction_id', 'departure_time'], observed=True)\n",
    "\n",
    "    # 4. Add a dummy column to each group\n",
    "    for _, group in groups:\n",
    "        group['dummy_column'] = range(len(group))\n",
    "        for i, row in group.iterrows():\n",
    "            row['block_id'] = row['block_id'][row['dummy_column']]\n",
    "            # replace the original row with the modified one\n",
    "            group.loc[i] = row\n",
    "        # Assign the new block_id values to the original dataframe\n",
    "        df.loc[group.index, 'block_id'] = group['block_id']\n",
    "    return df\n",
    "\n",
    "# From the folder, import calendar_csv\n",
    "def import_calendar_csv(foldername, filename):\n",
    "    calendar_csv_path = os.path.join(foldername, filename)\n",
    "    calendar_df = pd.read_csv(calendar_csv_path, sep=',')\n",
    "    # Convert the date column to datetime\n",
    "    calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%Y-%m-%d')\n",
    "    # Convert day_of_week to category\n",
    "    calendar_df['day_of_week'] = calendar_df['day_of_week'].astype('category')\n",
    "    # Convert service_ids to dictionary\n",
    "    calendar_df['service_ids'] = calendar_df['service_ids'].apply(eval)\n",
    "    return calendar_df\n",
    "\n",
    "def polish_arrival_departure_time_df(ArrDepDF, route_list):\n",
    "    # Extract records whose point_type equal to either 'Startpoint' or 'Endpoint', if route_list is not empty, filter the records by route_id\n",
    "    if route_list:\n",
    "        start_end_points_mask = (ArrDepDF.point_type.isin(['Startpoint', 'Endpoint'])) & (ArrDepDF.route_id.isin(route_list))\n",
    "    else:\n",
    "        start_end_points_mask = (ArrDepDF.point_type.isin(['Startpoint', 'Endpoint']))\n",
    "\n",
    "    ArrDepDF = ArrDepDF.loc[start_end_points_mask]\n",
    "    # If there are any nan values in the actual columns, replace them with the scheduled values\n",
    "    ArrDepDF.loc[:,'actual'] = ArrDepDF['actual'].fillna(ArrDepDF['scheduled'])\n",
    "    # Drop the columns that are not needed for the analysis\n",
    "    drop_columns = ['scheduled_headway', 'headway', 'route_short_name']\n",
    "    ArrDepDF = ArrDepDF.drop(columns=drop_columns)\n",
    "    ArrDepDF = ArrDepDF.reset_index(drop=True)\n",
    "    # Rename actual to departure_time\n",
    "    ArrDepDF = ArrDepDF.rename(columns={'scheduled': 'departure_time'})\n",
    "    # Replace 'Inbound' entries with 1 and 'Outbound' entries with 0\n",
    "    ArrDepDF['direction_id'] = ArrDepDF['direction_id'].cat.rename_categories({'Inbound': 1, 'Outbound': 0})\n",
    "    # Convert service_date, scheduled and departure_time to datetime objects\n",
    "    #ArrDepDF['service_date'] = pd.to_datetime(ArrDepDF['service_date'], format='%Y-%m-%d')\n",
    "    #ArrDepDF['departure_time'] = pd.to_datetime(ArrDepDF['departure_time'], format='%H:%M:%S')\n",
    "    #ArrDepDF.loc[:,'actual'] = pd.to_datetime(ArrDepDF.loc[:,'actual'], format='%H:%M:%S')\n",
    "    # Add a new service_id column to the ArrDepDF dataframe made of empty sets\n",
    "    ArrDepDF['service_id'] =''\n",
    "    # Add an empty block_id column\n",
    "    ArrDepDF['block_id'] = ''\n",
    "    return ArrDepDF\n",
    "\n",
    "def handle_24h_time(series):\n",
    "    return series.str.replace(r'^24', '00', regex=True) \\\n",
    "                 .str.replace(r'^25', '01', regex=True) \\\n",
    "                 .str.replace(r'^26', '02', regex=True) \\\n",
    "                 .str.replace(r'^27', '03', regex=True) \\\n",
    "                 .str.replace(r'^28', '04', regex=True)\n",
    "\n",
    "# Parse datetime strings to datetime objects\n",
    "def parse_datetime_strings(df):\n",
    "    df['arrival_time'] = handle_24h_time(df['arrival_time'])\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%H:%M:%S')\n",
    "    df['departure_time'] = handle_24h_time(df['departure_time'])\n",
    "    df['departure_time'] = pd.to_datetime(df['departure_time'], format='%H:%M:%S')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "gtfs_cols = {\n",
    "    'calendar': {\n",
    "    'service_id': 'category',\n",
    "    'monday': bool,\n",
    "    'tuesday': bool,\n",
    "    'wednesday': bool,\n",
    "    'thursday': bool,\n",
    "    'friday': bool,\n",
    "    'saturday': bool,\n",
    "    'sunday': bool,\n",
    "    'start_date': 'string',\n",
    "    'end_date': 'string'\n",
    "    },\n",
    "\n",
    "    'calendar_attributes':{\n",
    "    'service_id': 'category',\n",
    "    'service_description': 'category',\n",
    "    'service_schedule_name': 'category',\n",
    "    'service_schedule_type': 'category',\n",
    "    'service_schedule_typicality': pd.Int8Dtype(),\n",
    "    'rating_start_date': 'string',\n",
    "    'rating_end_date': 'string',\n",
    "    'rating_description': 'category'\n",
    "    },\n",
    "\n",
    "    'calendar_dates':{\n",
    "    'service_id': 'category',\n",
    "    'date': 'string',\n",
    "    'exception_type': pd.Int8Dtype(),\n",
    "    'holiday_name': 'category'\n",
    "    },\n",
    "\n",
    "    'feed_info': {\n",
    "    'feed_publisher_name': 'category',\n",
    "    'feed_publisher_url': 'category',\n",
    "    'feed_lang': 'category',\n",
    "    'feed_start_date': 'string',\n",
    "    'feed_end_date': 'string',\n",
    "    'feed_version': 'category',\n",
    "    'feed_contact_email': 'category',\n",
    "    'feed_contact_url': 'category'\n",
    "    },\n",
    "\n",
    "    'routes':{\n",
    "    'route_id': 'string',\n",
    "    'agency_id': 'category',\n",
    "    'route_short_name': 'category',\n",
    "    'route_long_name': 'category',\n",
    "    'route_desc': 'category',\n",
    "    'route_type': 'category',\n",
    "    'route_url': 'category',\n",
    "    'route_color': 'category',\n",
    "    'route_text_color': 'category',\n",
    "    'route_sort_order': pd.Int16Dtype(),\n",
    "    'route_fare': 'category',\n",
    "    'line_id': 'category',\n",
    "    'listed_route': 'category'\n",
    "    },\n",
    "\n",
    "    'stop_times':{\n",
    "    'trip_id': 'string',\n",
    "    'arrival_time': 'string',\n",
    "    'departure_time': 'string',\n",
    "    'stop_id': 'category',\n",
    "    'stop_sequence': pd.Int16Dtype(),\n",
    "    'stop_headsign': 'category',\n",
    "    'pickup_type': 'category',\n",
    "    'drop_off_type': 'category',\n",
    "    'timepoint': pd.Int16Dtype(),\n",
    "    'checkpoint_id': 'category',\n",
    "    'continuous_pickup': 'category',\n",
    "    'continuous_drop_off': 'category'\n",
    "    },\n",
    "\n",
    "    'stops':{\n",
    "    'stop_id': 'string',\n",
    "    'stop_code': 'category',\n",
    "    'stop_name': 'category',\n",
    "    'stop_desc': 'category',\n",
    "    'stop_lat': pd.Float32Dtype(),\n",
    "    'stop_lon': pd.Float32Dtype(),\n",
    "    'zone_id': 'category',\n",
    "    'stop_url': 'category',\n",
    "    'level_id': 'category',\n",
    "    'location_type': 'category',\n",
    "    'municipality': 'category',\n",
    "    'on_street': 'category',\n",
    "    'at_street': 'category',\n",
    "    'parent_station': 'category',\n",
    "    'stop_timezone': 'category',\n",
    "    'wheelchair_boarding': 'category',\n",
    "    'platform_code': 'category',\n",
    "    'platofrm_name': 'category',\n",
    "    'stop_address': 'category',\n",
    "    'stop_city': 'category',\n",
    "    'stop_region': 'category',\n",
    "    'stop_postal_code': 'category',\n",
    "    'stop_country': 'category',\n",
    "    'stop_phone': 'category',\n",
    "    'stop_url': 'category',\n",
    "    'stop_contact_name': 'category',\n",
    "    'stop_contact_phone': 'category',\n",
    "    'stop_contact_url': 'category',\n",
    "    'stop_contact_email': 'category',\n",
    "    'vehicle_type': 'category'\n",
    "    },\n",
    "\n",
    "    'trips':{\n",
    "    'route_id': 'string',\n",
    "    'service_id': 'string',\n",
    "    'trip_id': 'category',\n",
    "    'trip_headsign': 'category',\n",
    "    'trip_short_name': 'category',\n",
    "    'direction_id': 'category',\n",
    "    'block_id': 'category',\n",
    "    'shape_id': 'category',\n",
    "    'wheelchair_accessible': 'category',\n",
    "    'trip_route_type': 'category',\n",
    "    'route_pattern_id': 'category',\n",
    "    'bikes_allowed': 'category'}\n",
    "}\n",
    "\n",
    "schedule_cols = {\n",
    "    'trip_id': 'string',\n",
    "    'arrival_time': 'string',\n",
    "    'departure_time': 'string',\n",
    "    'stop_id': 'string',\n",
    "    'stop_sequence': pd.Int16Dtype(),\n",
    "    'timepoint': pd.Int16Dtype(),\n",
    "    'checkpoint_id': 'category',\n",
    "    'route_id': 'string',\n",
    "    'service_id': 'string',\n",
    "    'direction_id': 'category',\n",
    "    'block_id': 'string',\n",
    "    'rating_start_date': 'string',\n",
    "    'rating_end_date': 'string'\n",
    "}\n",
    "\n",
    "#Arrival adn Departure time columns\n",
    "adt_dtype_map = {\n",
    "    \"service_date\": \"string\",\n",
    "    \"route_id\": \"string\",\n",
    "    \"direction_id\": \"category\",\n",
    "    \"half_trip_id\": \"string\",\n",
    "    \"stop_id\": \"string\",\n",
    "    \"time_point_id\": \"category\", \n",
    "    \"time_point_order\": pd.Int16Dtype(),\n",
    "    \"point_type\": \"category\", \n",
    "    \"standard_type\": \"category\",  \n",
    "    \"scheduled\": \"string\",  # Consider converting to datetime later\n",
    "    \"actual\": \"string\",  # Consider converting to datetime later\n",
    "    \"scheduled_headway\": pd.Int32Dtype(),\n",
    "    \"headway\": pd.Int32Dtype()\n",
    "    }\n",
    "\n",
    "# Drop the columns that are not needed for the analysis\n",
    "\n",
    "drop_colums = ['trip_headsign', 'trip_short_name',\n",
    "        'shape_id', 'wheelchair_accessible',\n",
    "       'trip_route_type', 'route_pattern_id', 'bikes_allowed', 'stop_headsign',\n",
    "       'pickup_type', 'drop_off_type', \n",
    "       'continuous_pickup', 'continuous_drop_off']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>direction_id</th>\n",
       "      <th>block_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-1</td>\n",
       "      <td>04:37:00</td>\n",
       "      <td>04:37:00</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-1</td>\n",
       "      <td>05:04:00</td>\n",
       "      <td>05:04:00</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-3</td>\n",
       "      <td>04:51:00</td>\n",
       "      <td>04:51:00</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-3</td>\n",
       "      <td>05:18:00</td>\n",
       "      <td>05:18:00</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811660</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-7</td>\n",
       "      <td>05:05:00</td>\n",
       "      <td>05:05:00</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133481</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004493</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-124</td>\n",
       "      <td>16:47:00</td>\n",
       "      <td>16:47:00</td>\n",
       "      <td>5271</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133482</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004494</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>17:40:00</td>\n",
       "      <td>17:40:00</td>\n",
       "      <td>15058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133483</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004494</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>18:12:00</td>\n",
       "      <td>18:12:00</td>\n",
       "      <td>5271</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133484</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004495</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>19:05:00</td>\n",
       "      <td>19:05:00</td>\n",
       "      <td>15058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133485</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004495</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>19:32:00</td>\n",
       "      <td>19:32:00</td>\n",
       "      <td>5271</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133486 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             service_id   trip_id route_id direction_id  block_id  \\\n",
       "0       BUS123-3-Wdy-02  54811655        1            0     C01-1   \n",
       "1       BUS123-3-Wdy-02  54811655        1            0     C01-1   \n",
       "2       BUS123-3-Wdy-02  54811657        1            0     C01-3   \n",
       "3       BUS123-3-Wdy-02  54811657        1            0     C01-3   \n",
       "4       BUS123-3-Wdy-02  54811660        1            0     C01-7   \n",
       "...                 ...       ...      ...          ...       ...   \n",
       "133481    WinterWeekday  55004493       99            1  G110-124   \n",
       "133482    WinterWeekday  55004494       99            1  G110-118   \n",
       "133483    WinterWeekday  55004494       99            1  G110-118   \n",
       "133484    WinterWeekday  55004495       99            1  G110-118   \n",
       "133485    WinterWeekday  55004495       99            1  G110-118   \n",
       "\n",
       "       arrival_time departure_time stop_id  stop_sequence  \n",
       "0          04:37:00       04:37:00      64              1  \n",
       "1          05:04:00       05:04:00     110             24  \n",
       "2          04:51:00       04:51:00      64              1  \n",
       "3          05:18:00       05:18:00     110             24  \n",
       "4          05:05:00       05:05:00      64              1  \n",
       "...             ...            ...     ...            ...  \n",
       "133481     16:47:00       16:47:00    5271             38  \n",
       "133482     17:40:00       17:40:00   15058              1  \n",
       "133483     18:12:00       18:12:00    5271             38  \n",
       "133484     19:05:00       19:05:00   15058              1  \n",
       "133485     19:32:00       19:32:00    5271             38  \n",
       "\n",
       "[133486 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all *.txt files in the gtfsSchedule folder and parse them as dataframes\n",
    "# add the gtfsSchedule folder and subfolders to the current path\n",
    "txt_path = os.path.join('gtfsSchedule','gtfs_2022-12-18_2023-03-11_Winter2023PostRecap')\n",
    "# List only the files needed later, avoid to import the whole folder\n",
    "txt_list = (['calendar.txt', 'calendar_attributes.txt', 'calendar_dates.txt', 'feed_info.txt', 'routes.txt', 'stop_times.txt', 'stops.txt', 'trips.txt'])\n",
    "# List of dataframe names: remove the '.txt' extension from the filenames\n",
    "df_names = [txt_file.rstrip('.txt') for txt_file in txt_list]\n",
    "# Read txt files into dataframes and assign them the names in df_names, and the dtypes in gtfs_cols using the keys with the same name as the dataframe\n",
    "dfs = [pd.read_csv(os.path.join(txt_path, gtfs_file), sep=',', low_memory=False, dtype=gtfs_cols[df_name]) for df_name, gtfs_file in zip(df_names, txt_list)]\n",
    "#dfs = [pd.read_csv(os.path.join(txt_path, gtfs_file), sep=',', low_memory=False) for gtfs_file in txt_list]\n",
    "# create a dictionary of dataframes\n",
    "gtfsSchedule = dict(zip(df_names, dfs))\n",
    "# Assign the dataframes to variables\n",
    "calendar = gtfsSchedule['calendar']\n",
    "calendar_attributes = gtfsSchedule['calendar_attributes']\n",
    "calendar_dates = gtfsSchedule['calendar_dates']\n",
    "feed_info = gtfsSchedule['feed_info']\n",
    "routes = gtfsSchedule['routes']\n",
    "stop_times = gtfsSchedule['stop_times']\n",
    "end_points_df = (stop_times.groupby\n",
    "                     (['trip_id'], observed=True, as_index=False)\n",
    "                    ['stop_sequence']\n",
    "                    .transform('max') #with transform I can get all the max occurrences while also preserving their original row index\n",
    "                )\n",
    "stop_times = stop_times.loc[(stop_times.loc[:,'stop_sequence'] == end_points_df)|(stop_times.loc[:,'stop_sequence'] == 1)]\n",
    "stops = gtfsSchedule['stops']\n",
    "trips = gtfsSchedule['trips']\n",
    "\n",
    "# Filter routes and trips to only include bus routes, i.e., those whose rotue_id is a string of digits\n",
    "routes, trips = routes[routes['route_id'].str.isdigit()], trips[trips['route_id'].str.isdigit()]\n",
    "\n",
    "# Convert datetime strings to proper datetime objects\n",
    "calendar['start_date'] = pd.to_datetime(calendar['start_date'], format='%Y%m%d')\n",
    "calendar['end_date'] = pd.to_datetime(calendar['end_date'], format='%Y%m%d')\n",
    "calendar_attributes['rating_start_date'] = pd.to_datetime(calendar_attributes['rating_start_date'], format='%Y%m%d')\n",
    "calendar_attributes['rating_end_date'] = pd.to_datetime(calendar_attributes['rating_end_date'], format='%Y%m%d')\n",
    "feed_info['feed_start_date'] = pd.to_datetime(feed_info['feed_start_date'], format='%Y%m%d')\n",
    "feed_info['feed_end_date'] = pd.to_datetime(feed_info['feed_end_date'], format='%Y%m%d')\n",
    "calendar_dates['date'] = pd.to_datetime(calendar_dates['date'], format='%Y%m%d')\n",
    "# Add the service_id field from the calendar dataframe to the trips dataframe, without including the other fields\n",
    "trips = pd.merge(trips[['service_id','trip_id', 'route_id', 'direction_id', 'block_id']], calendar_attributes[['service_id']], on='service_id')\n",
    "# Merge stop_times with trips\n",
    "schedule = pd.merge(trips,stop_times[['trip_id','arrival_time','departure_time','stop_id','stop_sequence']], on='trip_id', how='left')\n",
    "parse_datetime_strings(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>direction_id</th>\n",
       "      <th>block_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-1</td>\n",
       "      <td>1900-01-01 04:37:00</td>\n",
       "      <td>1900-01-01 04:37:00</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-1</td>\n",
       "      <td>1900-01-01 05:04:00</td>\n",
       "      <td>1900-01-01 05:04:00</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-3</td>\n",
       "      <td>1900-01-01 04:51:00</td>\n",
       "      <td>1900-01-01 04:51:00</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-3</td>\n",
       "      <td>1900-01-01 05:18:00</td>\n",
       "      <td>1900-01-01 05:18:00</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUS123-3-Wdy-02</td>\n",
       "      <td>54811660</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C01-7</td>\n",
       "      <td>1900-01-01 05:05:00</td>\n",
       "      <td>1900-01-01 05:05:00</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133481</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004493</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-124</td>\n",
       "      <td>1900-01-01 16:47:00</td>\n",
       "      <td>1900-01-01 16:47:00</td>\n",
       "      <td>5271</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133482</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004494</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>1900-01-01 17:40:00</td>\n",
       "      <td>1900-01-01 17:40:00</td>\n",
       "      <td>15058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133483</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004494</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>1900-01-01 18:12:00</td>\n",
       "      <td>1900-01-01 18:12:00</td>\n",
       "      <td>5271</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133484</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004495</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>1900-01-01 19:05:00</td>\n",
       "      <td>1900-01-01 19:05:00</td>\n",
       "      <td>15058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133485</th>\n",
       "      <td>WinterWeekday</td>\n",
       "      <td>55004495</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>G110-118</td>\n",
       "      <td>1900-01-01 19:32:00</td>\n",
       "      <td>1900-01-01 19:32:00</td>\n",
       "      <td>5271</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133486 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             service_id   trip_id route_id direction_id  block_id  \\\n",
       "0       BUS123-3-Wdy-02  54811655        1            0     C01-1   \n",
       "1       BUS123-3-Wdy-02  54811655        1            0     C01-1   \n",
       "2       BUS123-3-Wdy-02  54811657        1            0     C01-3   \n",
       "3       BUS123-3-Wdy-02  54811657        1            0     C01-3   \n",
       "4       BUS123-3-Wdy-02  54811660        1            0     C01-7   \n",
       "...                 ...       ...      ...          ...       ...   \n",
       "133481    WinterWeekday  55004493       99            1  G110-124   \n",
       "133482    WinterWeekday  55004494       99            1  G110-118   \n",
       "133483    WinterWeekday  55004494       99            1  G110-118   \n",
       "133484    WinterWeekday  55004495       99            1  G110-118   \n",
       "133485    WinterWeekday  55004495       99            1  G110-118   \n",
       "\n",
       "              arrival_time      departure_time stop_id  stop_sequence  \n",
       "0      1900-01-01 04:37:00 1900-01-01 04:37:00      64              1  \n",
       "1      1900-01-01 05:04:00 1900-01-01 05:04:00     110             24  \n",
       "2      1900-01-01 04:51:00 1900-01-01 04:51:00      64              1  \n",
       "3      1900-01-01 05:18:00 1900-01-01 05:18:00     110             24  \n",
       "4      1900-01-01 05:05:00 1900-01-01 05:05:00      64              1  \n",
       "...                    ...                 ...     ...            ...  \n",
       "133481 1900-01-01 16:47:00 1900-01-01 16:47:00    5271             38  \n",
       "133482 1900-01-01 17:40:00 1900-01-01 17:40:00   15058              1  \n",
       "133483 1900-01-01 18:12:00 1900-01-01 18:12:00    5271             38  \n",
       "133484 1900-01-01 19:05:00 1900-01-01 19:05:00   15058              1  \n",
       "133485 1900-01-01 19:32:00 1900-01-01 19:32:00    5271             38  \n",
       "\n",
       "[133486 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.loc[schedule.arrival_time.str.startswith('25')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Assign to every row in trips the corresponding rating_start_date and rating_end_date contained in calendar_attributes matching the two datasets by service_id\n",
    "# Add the service_id field from the calendar dataframe to the trips dataframe, without including the other fields\n",
    "trips = pd.merge(trips[['service_id','trip_id', 'route_id', 'direction_id', 'block_id']], calendar_attributes[['service_id', 'rating_start_date', 'rating_end_date']], on='service_id')\n",
    "# Merge stop_times with trips\n",
    "schedule = pd.merge(trips,stop_times[['trip_id','arrival_time','departure_time','stop_id','stop_sequence']], on='trip_id', how='left').apply(parse_datetime_strings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schedule = parse_datetime_strings(schedule)\n",
    "schedule = schedule.drop(columns=drop_colums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files with arrival and departure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArrDepFolder = 'MBTA_ArrivalDepartureTimes'\n",
    "yearlyFolders = os.listdir(ArrDepFolder)\n",
    "numYears = len(yearlyFolders)\n",
    "\n",
    "#Empty dataframe\n",
    "adt_list = []\n",
    "chunksize = 10**5\n",
    "\n",
    "        \n",
    "for year in range(numYears):\n",
    "    num_files = len(os.listdir(os.path.join(ArrDepFolder, yearlyFolders[year])))\n",
    "    files_path = os.path.join(ArrDepFolder, yearlyFolders[year])\n",
    "\n",
    "    for month in range(num_files):\n",
    "        #print(f'files: {os.listdir(files_path)}')\n",
    "        filename = (os.path.join(files_path, os.listdir(files_path)[month]))\n",
    "        print(f'filename: {filename}')\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize, dtype=adt_dtype_map, low_memory=False):\n",
    "            #print('Step 1')\n",
    "            for key, value in adt_dtype_map.items():\n",
    "                chunk[key] = chunk[key].astype(value)\n",
    "            #print('Step 2')\n",
    "        adt_list.append(chunk)\n",
    "\n",
    "print('Concatenating...')\n",
    "adt_df = pd.concat(adt_list, axis=0)\n",
    "adt_df.to_csv('050224_adt_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace wrong route_ids with the correct one, if needed\n",
    "adt_df.loc[:,'route_id'] = adt_df.loc[:,'route_id'].str.rstrip('_')\n",
    "adt_df.loc[:,'half_trip_id'] = adt_df.loc[:,'half_trip_id'].str.rstrip('.0') #i guess .0 exists because there are some empty entries that are treated as nan\n",
    "adt_df['route_id'] = adt_df['route_id'].str.lstrip('0')          \n",
    "            #print('Step 3')\n",
    "\n",
    "# Convert service_date to datetime objects\n",
    "# Convert service_date, scheduled and actual columns to datetime objects\n",
    "adt_df['service_date'] = pd.to_datetime(adt_df['service_date'], format='%Y-%m-%d')\n",
    "adt_df['scheduled'] = pd.to_datetime(adt_df['scheduled'], format='ISO8601')\n",
    "adt_df['actual'] = pd.to_datetime(adt_df['actual'], format='ISO8601')\n",
    "#adt_df[\"scheduled\"] = adt_df[\"scheduled\"].dt.strftime(\"%H:%M:%S\")\n",
    "#adt_df[\"actual\"] = adt_df[\"actual\"].dt.strftime(\"%H:%M:%S\")\n",
    "adt_df = pd.merge(adt_df, routes[['route_id', 'route_short_name']], on='route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv files from 2022 and 2023, cast them into a single dataframe, and filter out the bus routes included within the dates of the scheduled df\n",
    "# Import the csv files\n",
    "csv_path = 'MBTA_ArrivalDepartureTimes'\n",
    "foldername = 'MBTA_Bus_Arrival_Departure_Times'\n",
    "\n",
    "adt2022_list = []\n",
    "adt2023_list = []\n",
    "import_process = 1\n",
    "if import_process:\n",
    "    # Arrival/Departure times 2022\n",
    "    adt_2022 = os.path.join(csv_path, (foldername + '_' + '2022'))\n",
    "    csv2022_files = os.listdir(adt_2022)\n",
    "    # Arrival/Departure times 2023\n",
    "    adt_2023 = os.path.join(csv_path, (foldername + '_' + '2023'))\n",
    "    csv2023_files = os.listdir(adt_2023)\n",
    "    \n",
    "    for i in range(12):\n",
    "        print(i)\n",
    "        adt2022_list.append(pd.read_csv(os.path.join(adt_2022, csv2022_files[i]), sep=','))\n",
    "        adt2023_list.append(pd.read_csv(os.path.join(adt_2023, csv2023_files[i]), sep=','))\n",
    "\n",
    "    # Build a single dataframe\n",
    "    adt_df = pd.concat((pd.concat(adt2023_list, axis = 0), pd.concat(adt2022_list, axis = 0)), axis = 0)\n",
    "   \n",
    "    print('Step 1')\n",
    "    # Change the dtype of the columns included in the colTypes to their corresponding values\n",
    "    for key, value in adt_dtype_map.items():\n",
    "        adt_df[key] = adt_df[key].astype(value)\n",
    "\n",
    "    print('Step 2')    \n",
    "    # Replace wrong route_ids with the correct one, if needed\n",
    "    adt_df.loc[:,'route_id'] = adt_df.loc[:,'route_id'].str.rstrip('_')\n",
    "    adt_df.loc[:,'half_trip_id'] = adt_df.loc[:,'half_trip_id'].str.rstrip('.0') #i guess .0 exists because there are some empty entries that are treated as nan\n",
    "    adt_df['route_id'] = adt_df['route_id'].str.lstrip('0')\n",
    "    # Use the routes gtfs file to match route_ids in the adt dataframe with their univocal identifier\n",
    "    adt_df = pd.merge(adt_df, routes[['route_id', 'route_short_name']], on='route_id')\n",
    "    \n",
    "    print('Step 3')\n",
    "    # Convert service_date to datetime objects\n",
    "    # Convert service_date, scheduled and actual columns to datetime objects\n",
    "    adt_df['service_date'] = pd.to_datetime(adt_df['service_date'], format='%Y-%m-%d')\n",
    "    adt_df['scheduled'] = pd.to_datetime(adt_df['scheduled'], format='ISO8601')\n",
    "    adt_df['actual'] = pd.to_datetime(adt_df['actual'], format='ISO8601')\n",
    "    #adt_df[\"scheduled\"] = adt_df[\"scheduled\"].dt.strftime(\"%H:%M:%S\")\n",
    "    #adt_df[\"actual\"] = adt_df[\"actual\"].dt.strftime(\"%H:%M:%S\")\n",
    "    print('Step 4')\n",
    "    # Keep only the rows whose service_date is within the range of the scheduled df\n",
    "    feed_info = gtfsSchedule['feed_info']\n",
    "    start_date = pd.to_datetime(feed_info.feed_start_date.values, format='%Y%m%d')\n",
    "    end_date = pd.to_datetime(feed_info.feed_end_date.values, format='%Y%m%d')\n",
    "    # Keep only entries whose service_date is within the range start_date and end_date\n",
    "    adt_df = adt_df[(adt_df['service_date'] >= start_date[0]) & (adt_df['service_date'] <= end_date[0])]\n",
    "    print('Step 5')\n",
    "    adt_df.reset_index()\n",
    "    \n",
    "else:\n",
    "    # Read the file in separate chunks and concatenate them\n",
    "    chunk_size = 10**6\n",
    "    chunks = []\n",
    "    \n",
    "    for chunk in pd.read_csv('adt_df.csv', dtype=adt_dtype_map, chunksize=chunk_size):\n",
    "        chunks.append(chunk)\n",
    "    adt_df = pd.concat(chunks, axis=0, ignore_index=True)\n",
    "    # Convert service_date, scheduled and actual columns to datetime objects\n",
    "    adt_df['service_date'] = pd.to_datetime(adt_df['service_date'], format='%Y-%m-%d')\n",
    "    adt_df['scheduled'] = pd.to_datetime(adt_df['scheduled'], format='ISO8601')\n",
    "    adt_df['actual'] = pd.to_datetime(adt_df['actual'], format='ISO8601')\n",
    "    #adt_df[\"scheduled\"] = adt_df[\"scheduled\"].dt.strftime(\"%H:%M:%S\")\n",
    "    #adt_df[\"actual\"] = adt_df[\"actual\"].dt.strftime(\"%H:%M:%S\")\n",
    "    # If half_trip_id endswith '.0', trim this piece\n",
    "\n",
    "    # Replace wrong route_ids with the correct one, if needed\n",
    "    adt_df.loc[:,'route_id'] = adt_df.loc[:,'route_id'].str.rstrip('_')\n",
    "    adt_df.loc[:,'half_trip_id'] = adt_df.loc[:,'half_trip_id'].str.rstrip('.0') #i guess .0 exists because there are some empty entries that are treated as nan\n",
    "    adt_df['route_id'] = adt_df['route_id'].str.lstrip('0')\n",
    "    # Use the routes gtfs file to match route_ids in the adt dataframe with their univocal identifier\n",
    "    routes = gtfsSchedule['routes']  \n",
    "    adt_df = pd.merge(adt_df, routes[['route_id', 'route_short_name']], on='route_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine arrival and departure times with scheduled information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = import_calendar_csv('CalendarDates', 'calendar_df.csv')\n",
    "adt_2 = polish_arrival_departure_time_df(adt_df, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign service_id and block_id to arrival and departure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adt_route10 and schedule on the route_id and departure_time columns\n",
    "schedule_route10 = schedule.copy()\n",
    "# Extract only the records with route_id == 10 and stop_sequence equal to 1\n",
    "schedule_route10 = schedule_route10.loc[(schedule_route10.route_id == '10') & (schedule_route10.stop_sequence == 1)]\n",
    "\n",
    "grouping_var = ['route_id', 'direction_id', 'departure_time'] # these are the same for both adt_route10 and schedule_route10\n",
    "# Work with only Startpoint records in arrival departure times, will merge with Endpoint records later\n",
    "startpoint_df = adt_route10.loc[adt_route10.point_type == 'Startpoint']\n",
    "adt_route10_grouped = startpoint_df.groupby(grouping_var, observed=True)\n",
    "schedule_route10_grouped = schedule_route10.groupby(grouping_var, observed=True)\n",
    "\n",
    "# Create two lists to store the unmatched names and groups\n",
    "unmatched_names = []\n",
    "unmatched_groups = []\n",
    "\n",
    "for name, group in adt_route10_grouped:\n",
    "    # print the group if an error occurs\n",
    "    if name in schedule_route10_grouped.groups:\n",
    "        # extract the corresponding group from schedule_route10_grouped\n",
    "        schedule_group = schedule_route10_grouped.get_group(name)\n",
    "        schedule_services = set(schedule_group['service_id'])\n",
    "        schedule_service_block_ids = schedule_group.groupby(['service_id'])['block_id'].apply(list)\n",
    "\n",
    "        # extract the subset of the calendar_df that matches the service_date\n",
    "        service_days = calendar_df.loc[calendar_df.date.isin(group.service_date)]\n",
    "        # loop through the service_days\n",
    "        for i, row in service_days.iterrows():\n",
    "            # add the intersection between schedule_services and row['service_ids'] to the service_id column in adt_route10 as plain strings\n",
    "            adt_service_ids = schedule_services.intersection(row['service_ids'])\n",
    "            adt_service_ids_str = ', '.join(adt_service_ids)  # Convert set to string\n",
    "            adt_route10.loc[group.index, 'service_id'] = adt_service_ids_str\n",
    "            # Get the block_id list associated to adt_service_ids_str\n",
    "            block_list=schedule_service_block_ids[schedule_service_block_ids.index==adt_service_ids_str]\n",
    "            block_list = block_list.iloc[0] if not block_list.empty else ''\n",
    "\n",
    "            if block_list and not group.empty:\n",
    "                # Concatenate block_list elements into a comma-separated string\n",
    "                block_ids_str = ', '.join(block_list)\n",
    "                # Assign the concatenated string to the specified rows in the column\n",
    "                adt_route10.loc[group.index, 'block_id'] = block_ids_str\n",
    "    else:\n",
    "        unmatched_names.append(name)\n",
    "        unmatched_groups.append(group)\n",
    "\n",
    "adt_route10 = split_multiple_block_id(adt_route10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the same values of block_id and service_id to Endpoint records with the same half_trip_id\n",
    "trips_grouped = adt_route10.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    adt_route10.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute layover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layover_df as a copy\n",
    "layover_df = adt_route10.copy()\n",
    "layover_df = layover_df.sort_values(by=['block_id','service_date','half_trip_id','departure_time'])\n",
    "layover_df=layover_df.reset_index(drop=True)\n",
    "\n",
    "# Group by 'block_id' and 'service_date', skipping rows with null 'service_id' or 'block_id'\n",
    "grouped = layover_df.loc[layover_df.block_id.notna()].groupby(['block_id', 'service_date'])\n",
    "\n",
    "# Calculate theoretical and actual layover times using diff()\n",
    "layover_df['theoretical_layover'] = grouped['departure_time'].diff().dt.total_seconds() / 60\n",
    "layover_df['actual_layover'] = grouped['actual'].diff().dt.total_seconds() / 60\n",
    "\n",
    "# Replace the first row of each group with null timedelta\n",
    "layover_df.loc[grouped.head(1).index, ['theoretical_layover', 'actual_layover']] = 0\n",
    "layover_df.loc[layover_df.time_point_order != 1, ['theoretical_layover', 'actual_layover']] = np.nan\n",
    "layover_df = layover_df.drop(columns=['time_point_order', 'point_type', 'standard_type'])\n",
    "(layover_df.groupby(['block_id', 'service_date'])[['theoretical_layover', 'actual_layover']]\n",
    "            .agg({'theoretical_layover': ['mean', 'max', 'count'],\n",
    "                  'actual_layover': ['mean', 'max', 'count'],\n",
    "                  'stop_id': 'first'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = adt_route10.groupby(['half_trip_id'])\n",
    "startpoint_df = test.get_group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
