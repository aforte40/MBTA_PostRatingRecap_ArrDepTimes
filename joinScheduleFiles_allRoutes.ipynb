{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_WARN_SLOW_RESOLVE_TIMEOUT'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Set the option to prevent the FutureWarning\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import datetime as dt\n",
    "from functions import *\n",
    "from dtype_dictionaries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position of the gtfs post-rating schedule files\n",
    "gtfs_post_rating_files = os.listdir('gtfsSchedule')\n",
    "# Sort the elements in alphabetical order. I need to ensure this otherwise the strategy of keeping the latest df in memory won't work\n",
    "gtfs_post_rating_files.sort()\n",
    "\n",
    "# List only the files needed later, avoid to import the whole folder\n",
    "txt_list = (['calendar.txt', 'calendar_attributes.txt', 'calendar_dates.txt', 'feed_info.txt', 'routes.txt', 'stop_times.txt', 'stops.txt', 'trips.txt'])\n",
    "# Position of MBTA_ArrivalDepartureTimes files\n",
    "ArrDepFolder = 'MBTA_ArrivalDepartureTimes'\n",
    "# Position of the  parsed ArrivalDepartureTimes files\n",
    "parsed_ArrDepFolder = 'parsed_ArrivalDepartureFiles'\n",
    "# List of files contained in the parsed_ArrivalDepartureFiles folder\n",
    "parsed_gtfs_rt = os.listdir(parsed_ArrDepFolder)\n",
    "parsed_gtfs_rt.sort()\n",
    "\n",
    "chunk_size = 12**5\n",
    "\n",
    "# Initialize to store calendar days and related services, to be converted to dataframe\n",
    "calendar_service_map = []\n",
    "# Initialize the list of dataframes to be concatenated containing all the data from gtfs rt\n",
    "df_ArrDep_list = []\n",
    "# Initialize the list of gtfs_schedules to be concatenated\n",
    "gtfs_schedules_list = []\n",
    "\n",
    "start_date_list = []\n",
    "end_date_list = []\n",
    "# Read files or import table?\n",
    "import_separate_files = False\n",
    "\n",
    "# Create two lists to store the unmatched names and groups\n",
    "unmatched_names = []\n",
    "unmatched_groups = []\n",
    "\n",
    "# Variable we wish to groupby\n",
    "grouping_vars = ['direction_id', 'scheduled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files with arrival and departure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 01_gtfs_post-recap_Winter_2022...\n",
      "Importing 20211219_20220312.csv...\n",
      "Processing route 1...\n",
      "Feed start date: 2021-12-19 00:00:00, Feed end date: 2022-03-12 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antonio.forte\\Dropbox (MIT)\\GitHub\\getGTFS-RT\\MBTA_PostRatingRecap_ArrDepTimes\\functions.py:288: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['WinterSunday', 'ChristmasDay-NewYearsDay', 'WinterSunday', 'ChristmasDay-NewYearsDay', 'WinterSunday', ..., 'WinterSunday', 'WinterSunday', 'WinterSunday', 'WinterSunday', 'WinterSunday']\n",
      "Length: 14\n",
      "Categories (129, object): ['BUS122-hba12ns1-Wdy-02', 'BUS122-hbb12ns1-Wdy-02', 'BUS122-hbc12ns1-Wdy-02', 'BUS122-hbf12ns1-Wdy-02', ..., 'RTL122-hms12fn1-Wdy-01', 'WinterSaturday', 'WinterSunday', 'WinterWeekday']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[merged.index, 'service_id'] = merged['service_id_y']\n",
      "c:\\Users\\antonio.forte\\Dropbox (MIT)\\GitHub\\getGTFS-RT\\MBTA_PostRatingRecap_ArrDepTimes\\functions.py:289: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[list(['C01-11']) list(['C01-11']) list(['C01-11']) list(['C01-11'])\n",
      " list(['C01-11']) list(['C01-11']) list(['C01-11']) list(['C01-11'])\n",
      " list(['C01-11']) list(['C01-11']) list(['C01-11']) list(['C01-11'])\n",
      " list(['C01-11']) list(['C01-11'])]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[merged.index, 'block_id'] = merged['block_id_y']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m         adt_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(adt_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# Map realtime data to gtfs schedule to assign block_ids and service_ids\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m         adt_df \u001b[38;5;241m=\u001b[39m \u001b[43mmap_realtime_to_gtfs_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43madt_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalendar_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtfs_schedule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcatenating...\u001b[39m\u001b[38;5;124m'\u001b[39m)      \n\u001b[0;32m     57\u001b[0m adt_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(adt_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\antonio.forte\\Dropbox (MIT)\\GitHub\\getGTFS-RT\\MBTA_PostRatingRecap_ArrDepTimes\\functions.py:297\u001b[0m, in \u001b[0;36mmap_realtime_to_gtfs_schedule\u001b[1;34m(df, start_date, end_date, calendar_schedule, gtfs_schedule)\u001b[0m\n\u001b[0;32m    294\u001b[0m         unmatched_names\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[0;32m    295\u001b[0m         unmatched_groups\u001b[38;5;241m.\u001b[39mappend(group)\n\u001b[1;32m--> 297\u001b[0m export_df \u001b[38;5;241m=\u001b[39m \u001b[43madd_info_to_endpoint_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute_id\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mroute\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Save the route-specific df to a csv file\u001b[39;00m\n\u001b[0;32m    300\u001b[0m export_filename \u001b[38;5;241m=\u001b[39m route \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\antonio.forte\\Dropbox (MIT)\\GitHub\\getGTFS-RT\\MBTA_PostRatingRecap_ArrDepTimes\\functions.py:313\u001b[0m, in \u001b[0;36madd_info_to_endpoint_rows\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    311\u001b[0m endpoint_df \u001b[38;5;241m=\u001b[39m endpoint_df\u001b[38;5;241m.\u001b[39mmerge(startpoint_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf_trip_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice_id\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf_trip_id\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf_trip_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# Concatenate the two dataframes horizontally\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mconcat([startpoint_df,endpoint_df])\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n",
      "File \u001b[1;32mc:\\Users\\antonio.forte\\Dropbox (MIT)\\GitHub\\getGTFS-RT\\MBTA_PostRatingRecap_ArrDepTimes\\functions.py:313\u001b[0m, in \u001b[0;36madd_info_to_endpoint_rows\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    311\u001b[0m endpoint_df \u001b[38;5;241m=\u001b[39m endpoint_df\u001b[38;5;241m.\u001b[39mmerge(startpoint_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf_trip_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice_id\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf_trip_id\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf_trip_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# Concatenate the two dataframes horizontally\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mconcat([startpoint_df,endpoint_df])\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_line:\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpydev_original_step_cmd)\n\u001b[1;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_return:  \u001b[38;5;66;03m# return event\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m     back \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for postRatingRecap_file in gtfs_post_rating_files:\n",
    "    adt_list = []\n",
    "    print(f'Processing {postRatingRecap_file}...')\n",
    "    gtfs_post_rating_folder = os.path.join('gtfsSchedule', postRatingRecap_file)\n",
    "    calendar, calendar_attributes, calendar_dates, feed_info, routes, stop_times, stops, trips, gtfs_schedule = get_gtfs_post_rating_txt_files(gtfs_post_rating_folder, txt_list, gtfs_cols)\n",
    "    # Add schedule to the list of gtfs_schedules\n",
    "    gtfs_schedules_list.append(gtfs_schedule)    \n",
    "    # Save feed_start_date and feed_end_date\n",
    "    start_date = feed_info['feed_start_date'][0]\n",
    "    start_date_list.append(start_date)\n",
    "    end_date = feed_info['feed_end_date'][0]\n",
    "    end_date_list.append(end_date)\n",
    "    calendar_data = parse_calendar_file(calendar)\n",
    "    calendar_data = parse_calendar_dates_file(calendar_dates, calendar_data)\n",
    "    calendar_schedule = generate_schedule(start_date, end_date, calendar_data)\n",
    "    calendar_service_map.append(calendar_schedule)\n",
    "\n",
    "    if import_separate_files:\n",
    "\n",
    "        # Return the list of compatible files\n",
    "        compatibleFiles = get_compatible_files(ArrDepFolder, start_date, end_date)\n",
    "        for filename in (compatibleFiles):   \n",
    "            print(f'Importing {filename}...')  \n",
    "            for chunk in pd.read_csv(filename, chunksize=chunk_size, dtype=adt_dtype_map, low_memory=False):\n",
    "                # Carry out here any filtering, drop or cutting down operation\n",
    "                chunk = reduce_df_size(chunk)\n",
    "                chunk = adjust_adt_df_settings(chunk, routes, start_date, end_date)\n",
    "                adt_list.append(chunk)\n",
    "\n",
    "        print('Concatenating...')\n",
    "        #adt_df = pd.concat(adt_list, axis=0)\n",
    "        # Filter out all the records that lie outside the feed_start_date and feed_end_date rang\n",
    "        #adt_df = adjust_adt_df_settings(adt_df, routes, start_date, end_date)\n",
    "        # Save the dataframe to a csv file: filename is equal as 'feed_start_date_feed_end_date.csv'\n",
    "        export_filename = start_date.strftime('%Y%m%d') + '_' + end_date.strftime('%Y%m%d') + '.csv'\n",
    "        filepath = os.path.join('parsed_ArrivalDepartureFiles', export_filename)\n",
    "        print(f'Exporting {filepath}...')\n",
    "        pd.concat(adt_list, axis=0).to_csv(filepath, index=False)\n",
    "\n",
    "    else:        \n",
    "        # Get list index of postRatingRecap_file\n",
    "        idx = gtfs_post_rating_files.index(postRatingRecap_file)\n",
    "        parsed_file = parsed_gtfs_rt[idx]\n",
    "        print(f'Importing {parsed_file}...') \n",
    "        filepath = os.path.join(parsed_ArrDepFolder, parsed_file)\n",
    "        for chunk in pd.read_csv(filepath, chunksize=chunk_size, dtype=adt_dtype_map, low_memory=False):\n",
    "            chunk['service_date'] = pd.to_datetime(chunk['service_date'], format='%Y-%m-%d')\n",
    "            chunk['scheduled'] = pd.to_datetime(chunk['scheduled'], format='%H:%M:%S')\n",
    "            chunk['actual'] = pd.to_datetime(chunk['actual'], format='%H:%M:%S')            \n",
    "            adt_list.append(chunk)\n",
    "\n",
    "        # Concatenate the list of dataframes\n",
    "        adt_df = pd.concat(adt_list, axis=0, ignore_index=True)\n",
    "        # Map realtime data to gtfs schedule to assign block_ids and service_ids\n",
    "        adt_df = map_realtime_to_gtfs_schedule(adt_df, start_date, end_date, calendar_schedule, gtfs_schedule)\n",
    "print('Concatenating...')      \n",
    "adt_df = pd.concat(adt_list, axis=0, ignore_index=True)\n",
    "# Build the calendar_service_map and gtfs_schedule dataframe\n",
    "# Convert every element in calendar_service_map to a dataframe\n",
    "calendar_df = pd.concat([pd.DataFrame(map, columns=['date', 'day_of_week', 'service_ids']) for map in calendar_service_map], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine arrival and departure times with scheduled information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_df.block_id = ''\n",
    "adt_df.service_id = ''\n",
    "adt_df_copy = adt_df.copy()\n",
    "\n",
    "route_ids = adt_df_copy['route_id'].unique()\n",
    "# From the start and end date lists, extract the feed_start_date and feed_end_date and convert them to datetime ranges\n",
    "feed_start_date = pd.to_datetime(start_date_list, format='%Y%m%d')\n",
    "feed_end_date = pd.to_datetime(end_date_list, format='%Y%m%d')\n",
    "\n",
    "for route in route_ids[0:1]:\n",
    "    print(f'Processing route {route}...')\n",
    "    # Fetch subset of the ArrivalDepartureTimes dataframe for the current route and stop_sequence ==1\n",
    "    adt_route = adt_df_copy.loc[(adt_df_copy['route_id'] == route) & (adt_df_copy['point_type'] == 'Startpoint')]\n",
    "\n",
    "    for feeds in zip(feed_start_date[0:1], feed_end_date[0:1], calendar_service_map[0:1], gtfs_schedules_list[0:1]):\n",
    "        # Print the feed_start_date and feed_end_date\n",
    "        print(f'Feed start date: {feeds[0]}, Feed end date: {feeds[1]}')\n",
    "        #Filter adt_df to keep only the rows that lie within the feed_start_date and feed_end_date range\n",
    "        date_filter = (adt_route['service_date'] >= feeds[0]) & (adt_route['service_date'] <= feeds[1])\n",
    "        adt_grouped = adt_route[date_filter]\n",
    "        service_map = feeds[2]\n",
    "        gtfs_schedule = feeds[3]\n",
    "        # Fetch subset of the GTFS schedule dataframe for the current route\n",
    "        schedule_route = gtfs_schedule[(gtfs_schedule['route_id'] == route)&(gtfs_schedule['stop_sequence'] == 1)]\n",
    "\n",
    "        adt_grouped = adt_grouped.groupby(grouping_vars, observed=True)\n",
    "        schedule_grouped = schedule_route.groupby(grouping_vars, observed=True)\n",
    "    \n",
    "        for name, group in adt_grouped:\n",
    "            # print the group name\n",
    "            print(f'{name}...')\n",
    "            if name in schedule_grouped.groups:\n",
    "                # extract the corresponding group from schedule_route10_grouped\n",
    "                schedule_group = schedule_grouped.get_group(name)\n",
    "                schedule_services = set(schedule_group['service_id'])\n",
    "                # This is a series whose index is the service_id and the values are the block_ids\n",
    "                schedule_service_block_ids = schedule_group.groupby(['service_id'], observed=True, as_index=False)['block_id'].apply(list)\n",
    "\n",
    "                # extract the subset of the calendar_df that matches the service_date\n",
    "                service_days_orig = calendar_df.loc[calendar_df.date.isin(group.service_date)]\n",
    "\n",
    "                # loop through the service_days\n",
    "                # Step 1: Compute the intersections\n",
    "                service_days = service_days_orig.copy()\n",
    "                service_days.loc[:,'adt_service_ids'] = service_days['service_ids'].apply(lambda ids: schedule_services.intersection(ids))\n",
    "                service_days.loc[:,'adt_service_ids_str'] = service_days['adt_service_ids'].apply(lambda ids: ', '.join(ids))\n",
    "\n",
    "                # Step 2: Merge service_days with group on date \n",
    "                merged = pd.merge(group, service_days, left_on='service_date', right_on='date')\n",
    "                # Step 3: Merge the merged dataframe with schedule_service_block_ids and keep the index of the group\n",
    "                merged = pd.merge(merged, schedule_service_block_ids, left_on='adt_service_ids_str', right_on='service_id')\n",
    "                # Print the name of the group if the length of the two indexes is different to spot potential errors\n",
    "                if len(merged.index) != len(group.index):\n",
    "                    print(f'Route: {route}\\n Length of indexes for group {name} is different: {len(merged.index)} vs {len(group.index)}')\n",
    "                    # Drop group rows whose service_date is not in the service_date of merged\n",
    "                    group = group[group['service_date'].isin(merged['service_date'])]\n",
    "\n",
    "                merged = merged.set_index(group.index)\n",
    "                # Step 4: recast the service_id and block_id in the adt_df\n",
    "                adt_df_copy.loc[merged.index, 'service_id'] = merged['service_id_y'] \n",
    "                adt_df_copy.loc[merged.index, 'block_id'] = merged['block_id_y']\n",
    "\n",
    "            else:\n",
    "                #print(f'No match found for group {name}...')\n",
    "                unmatched_names.append(name)\n",
    "                unmatched_groups.append(group)\n",
    "\n",
    "    # Save the route-specific df to a csv file\n",
    "    export_filename = route + '.csv'\n",
    "    filepath = os.path.join('block_ids_df', export_filename)\n",
    "    # Keep the index in the exported csv file and its name half_trip_id\n",
    "    #adt_df_copy.loc[(adt_df_copy.route_id == route) & (adt_df_copy['point_type'] == 'Startpoint')].to_csv(filepath, index=True)\n",
    "    endpoint_df = adt_df_copy.loc[adt_df_copy['point_type'] == 'Endpoint']\n",
    "    startpoint_df = adt_df_copy.loc[adt_df_copy['point_type'] == 'Startpoint', ['half_trip_id','block_id', 'service_id']]\n",
    "    endpoint_df.merge(startpoint_df, how='left', left_index=True, right_index=True, suffixes=('_end', '_start'))\n",
    "# I want to extract the subset containing only Endpoint rows that do not have any block_id nor service_id\n",
    "# I will subsequently merge this df with that containing only Startpoint that was just processed (it takes much less time)\n",
    "# And finally, I will concatenate the two df horizontally and obtain the final df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_route1_copy = adt_route1.copy()\n",
    "endpoint_df = adt_route1_copy.loc[adt_route1_copy['point_type'] == 'Endpoint']\n",
    "# Drop block_id and service_id columns\n",
    "endpoint_df = endpoint_df.drop(columns=['block_id', 'service_id'])\n",
    "startpoint_df = adt_route1_copy.loc[adt_route1_copy['point_type'] == 'Startpoint', ['half_trip_id','block_id', 'service_id']]\n",
    "startpoint_df\n",
    "endpoint_df = endpoint_df.merge(startpoint_df, left_on='half_trip_id', right_on='half_trip_id')\n",
    "# Concatenate the two dataframes horizontally\n",
    "final_df = pd.concat([startpoint_df,endpoint_df], axis=1)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in service_days.iterrows():\n",
    "                    # add the intersection between schedule_services and row['service_ids'] to the service_id column in adt_route10 as plain strings\n",
    "                    adt_service_ids = schedule_services.intersection(row['service_ids'])\n",
    "                    adt_service_ids_str = ', '.join(adt_service_ids)  # Convert set to string\n",
    "                    calendar_idx = group.loc[group.service_date == row.date].index\n",
    "                    half_trip_id_value = adt_df.loc[calendar_idx, 'half_trip_id'].values[0]\n",
    "                    # Assign service_id to row with Startpoint\n",
    "                    adt_df.loc[calendar_idx, 'service_id'] = adt_service_ids_str\n",
    "                    # Assign half_trip_id to row with Endpoint --> this is the same as the half_trip_id of the corresponding Startpoint\n",
    "                    adt_df.loc[adt_df.half_trip_id == half_trip_id_value, 'service_id'] = adt_service_ids_str\n",
    "                    # Get the block_id list associated to adt_service_ids_str\n",
    "                    block_list=schedule_service_block_ids[schedule_service_block_ids.index==adt_service_ids_str]\n",
    "                    block_list = block_list.iloc[0] if not block_list.empty else ''\n",
    "\n",
    "                    if block_list and not group.empty:\n",
    "                        # Concatenate block_list elements into a comma-separated string\n",
    "                        block_ids_str = ', '.join(block_list)\n",
    "                        # Assign the concatenated string to the Startpoint row\n",
    "                        adt_df.loc[calendar_idx, 'block_id'] = block_ids_str\n",
    "                        # Assign the concatenated string to the Endpoint row\n",
    "                        adt_df.loc[adt_df.half_trip_id == half_trip_id_value, 'block_id'] = block_ids_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_df_copy.loc[adt_df_copy.route_id=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Split Blocks')\n",
    "adt_df_block_ids = split_multiple_block_id(adt_df_block_ids)\n",
    "print('Assign Blocks to endpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the same values of block_id and service_id to Endpoint records with the same half_trip_id\n",
    "trips_grouped = adt_df.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    if len(group.index) > 1:\n",
    "        adt_df.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values\n",
    "    else:\n",
    "        print(name)\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_grouped = adt_df.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    if len(group.index) > 1:\n",
    "        adt_df.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values\n",
    "    else:\n",
    "        print(name)\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the same values of block_id and service_id to Endpoint records with the same half_trip_id\n",
    "trips_grouped = adt_df.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    adt_df.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute layover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layover_df as a copy\n",
    "layover_df = adt_route10.copy()\n",
    "layover_df = layover_df.sort_values(by=['block_id','service_date','half_trip_id','departure_time'])\n",
    "layover_df=layover_df.reset_index(drop=True)\n",
    "\n",
    "# Group by 'block_id' and 'service_date', skipping rows with null 'service_id' or 'block_id'\n",
    "grouped = layover_df.loc[layover_df.block_id.notna()].groupby(['block_id', 'service_date'])\n",
    "\n",
    "# Calculate theoretical and actual layover times using diff()\n",
    "layover_df['theoretical_layover'] = grouped['departure_time'].diff().dt.total_seconds() / 60\n",
    "layover_df['actual_layover'] = grouped['actual'].diff().dt.total_seconds() / 60\n",
    "\n",
    "# Replace the first row of each group with null timedelta\n",
    "layover_df.loc[grouped.head(1).index, ['theoretical_layover', 'actual_layover']] = 0\n",
    "layover_df.loc[layover_df.time_point_order != 1, ['theoretical_layover', 'actual_layover']] = np.nan\n",
    "layover_df = layover_df.drop(columns=['time_point_order', 'point_type', 'standard_type'])\n",
    "(layover_df.groupby(['block_id', 'service_date'])[['theoretical_layover', 'actual_layover']]\n",
    "            .agg({'theoretical_layover': ['mean', 'max', 'count'],\n",
    "                  'actual_layover': ['mean', 'max', 'count'],\n",
    "                  'stop_id': 'first'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = adt_route10.groupby(['half_trip_id'])\n",
    "startpoint_df = test.get_group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
