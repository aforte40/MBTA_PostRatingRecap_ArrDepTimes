{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_WARN_SLOW_RESOLVE_TIMEOUT'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Set the option to prevent the FutureWarning\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import datetime as dt\n",
    "from functions import *\n",
    "from dtype_dictionaries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and process gtfs schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import all *.txt files in the gtfsSchedule folder and parse them as dataframes\n",
    "# add the gtfsSchedule folder and subfolders to the current path\n",
    "txt_path = os.path.join('gtfsSchedule','gtfs_2022-12-18_2023-03-11_Winter2023PostRecap')\n",
    "# List only the files needed later, avoid to import the whole folder\n",
    "txt_list = (['calendar.txt', 'calendar_attributes.txt', 'calendar_dates.txt', 'feed_info.txt', 'routes.txt', 'stop_times.txt', 'stops.txt', 'trips.txt'])\n",
    "# List of dataframe names: remove the '.txt' extension from the filenames\n",
    "df_names = [txt_file.rstrip('.txt') for txt_file in txt_list]\n",
    "# Read txt files into dataframes and assign them the names in df_names, and the dtypes in gtfs_cols using the keys with the same name as the dataframe\n",
    "dfs = [pd.read_csv(os.path.join(txt_path, gtfs_file), sep=',', low_memory=False, dtype=gtfs_cols[df_name]) for df_name, gtfs_file in zip(df_names, txt_list)]\n",
    "#dfs = [pd.read_csv(os.path.join(txt_path, gtfs_file), sep=',', low_memory=False) for gtfs_file in txt_list]\n",
    "# create a dictionary of dataframes\n",
    "gtfsSchedule = dict(zip(df_names, dfs))\n",
    "# Assign the dataframes to variables\n",
    "calendar = gtfsSchedule['calendar']\n",
    "calendar_attributes = gtfsSchedule['calendar_attributes']\n",
    "calendar_dates = gtfsSchedule['calendar_dates']\n",
    "feed_info = gtfsSchedule['feed_info']\n",
    "routes = gtfsSchedule['routes']\n",
    "stop_times = gtfsSchedule['stop_times']\n",
    "end_points_df = (stop_times.groupby\n",
    "                     (['trip_id'], observed=True, as_index=False)\n",
    "                    ['stop_sequence']\n",
    "                    .transform('max') #with transform I can get all the max occurrences while also preserving their original row index\n",
    "                )\n",
    "stop_times = stop_times.loc[(stop_times.loc[:,'stop_sequence'] == end_points_df)|(stop_times.loc[:,'stop_sequence'] == 1)]\n",
    "# Drop arrival_time and rename departure_time to scheduled\n",
    "stop_times = stop_times.drop(columns=['arrival_time'])\n",
    "stop_times = stop_times.rename(columns={'departure_time': 'scheduled'})\n",
    "stops = gtfsSchedule['stops']\n",
    "trips = gtfsSchedule['trips']\n",
    "\n",
    "# Filter routes and trips to only include bus routes, i.e., those whose rotue_id is a string of digits\n",
    "routes, trips = routes[routes['route_id'].str.isdigit()], trips[trips['route_id'].str.isdigit()]\n",
    "\n",
    "# Convert datetime strings to proper datetime objects\n",
    "calendar['start_date'] = pd.to_datetime(calendar['start_date'], format='%Y%m%d')\n",
    "calendar['end_date'] = pd.to_datetime(calendar['end_date'], format='%Y%m%d')\n",
    "calendar_attributes['rating_start_date'] = pd.to_datetime(calendar_attributes['rating_start_date'], format='%Y%m%d')\n",
    "calendar_attributes['rating_end_date'] = pd.to_datetime(calendar_attributes['rating_end_date'], format='%Y%m%d')\n",
    "start_date_str = feed_info['feed_start_date'].values[0]\n",
    "end_date_str = feed_info['feed_end_date'].values[0]\n",
    "feed_info['feed_start_date'] = pd.to_datetime(feed_info['feed_start_date'], format='%Y%m%d')\n",
    "feed_info['feed_end_date'] = pd.to_datetime(feed_info['feed_end_date'], format='%Y%m%d')\n",
    "calendar_dates['date'] = pd.to_datetime(calendar_dates['date'], format='%Y%m%d')\n",
    "# Add the service_id field from the calendar dataframe to the trips dataframe, without including the other fields\n",
    "trips = pd.merge(trips[['service_id','trip_id', 'route_id', 'direction_id', 'block_id']], calendar_attributes[['service_id']], on='service_id')\n",
    "# Merge stop_times with trips\n",
    "schedule = pd.merge(trips,stop_times[['trip_id','scheduled','stop_id','stop_sequence']], on='trip_id', how='left')\n",
    "schedule = parse_datetime_strings(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a listdir containg only files whose month and year are compatible with feed_start_date and feed_end_date\n",
    "# Start with creating the list of all the files included in the subfolders of ArrDepFolder\n",
    "ArrDepFolder = 'MBTA_ArrivalDepartureTimes'\n",
    "yearlyFolders = os.listdir(ArrDepFolder)\n",
    "numYears = len(yearlyFolders)\n",
    "file_list = []\n",
    "for year in range(numYears):\n",
    "    num_files = len(os.listdir(os.path.join(ArrDepFolder, yearlyFolders[year])))\n",
    "    files_path = os.path.join(ArrDepFolder, yearlyFolders[year])\n",
    "\n",
    "    for month in range(num_files):\n",
    "        #print(f'files: {os.listdir(files_path)}')\n",
    "        filename = (os.path.join(files_path, os.listdir(files_path)[month]))\n",
    "        file_list.append(filename)\n",
    "\n",
    "# This list will be used to filter the files to be imported\n",
    "compatibleFiles = []\n",
    "start_date = feed_info['feed_start_date'][0]\n",
    "end_date = feed_info['feed_end_date'][0]\n",
    "\n",
    "for file in file_list:\n",
    "    # Extract the date from the filename\n",
    "    date = file.split('_')[-1].split('.')[0]\n",
    "    # Convert the date to a datetime object\n",
    "    date = dt.datetime.strptime(date, '%Y-%m')\n",
    "    # Check if the date is within the feed_start_date and feed_end_date. Apply the control only to year and month\n",
    "    if dt.date(year=start_date.year, month=start_date.month, day=1) <= date.date() <= dt.date(year=end_date.year, month=end_date.month, day=31):\n",
    "        compatibleFiles.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files with arrival and departure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty dataframe\n",
    "adt_list = []\n",
    "chunk_size = 10**5\n",
    "\n",
    "# Read files or import table?\n",
    "import_separate_files = False\n",
    "i = 0\n",
    "\n",
    "filename = start_date_str + '_' + end_date_str + '.csv'\n",
    "filepath = os.path.join('parsed_ArrivalDepartureFiles', filename)\n",
    "\n",
    "if import_separate_files:        \n",
    "    for filename in (compatibleFiles):   \n",
    "        print(f'Importing {filename}...')  \n",
    "        for chunk in pd.read_csv(filename, chunksize=chunk_size, dtype=adt_dtype_map, low_memory=False):\n",
    "            # Carry out here any filtering, drop or cutting down operation\n",
    "            chunk = reduce_df_size(chunk)\n",
    "            # Parse datetime strings to datetime objects only once at the end\n",
    "            adt_list.append(chunk)\n",
    "           \n",
    "    print('Concatenating...')\n",
    "    adt_df = pd.concat(adt_list, axis=0)\n",
    "    # Filter out all the records that lie outside the feed_start_date and feed_end_date rang\n",
    "    adt_df = adjust_adt_df_settings(adt_df, routes, start_date, end_date)\n",
    "    # Save the dataframe to a csv file: filename is equal as 'feed_start_date_feed_end_date.csv'\n",
    "    filename = start_date_str + '_' + end_date_str + '.csv'\n",
    "    filepath = os.path.join('parsed_ArrivalDepartureFiles', filename)\n",
    "    adt_df.to_csv(filepath, index=False)\n",
    "else:\n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunk_size, dtype=adt_dtype_map, low_memory=False):\n",
    "        adt_list.append(chunk)\n",
    "    adt_df = pd.concat(adt_list, axis=0, ignore_index=True)\n",
    "    # Convert service_date, scheduled and actual columns to datetime objects\n",
    "    adt_df['service_date'] = pd.to_datetime(adt_df['service_date'], format='%Y-%m-%d')\n",
    "    adt_df['scheduled'] = pd.to_datetime(adt_df['scheduled'], format='%H:%M:%S')\n",
    "    adt_df['actual'] = pd.to_datetime(adt_df['actual'], format='%H:%M:%S')\n",
    "    #adt_df = adjust_adt_df_settings(adt_df, routes, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine arrival and departure times with scheduled information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the calendar_df with only the feasible service_ids for every service date\n",
    "calendar_df = import_calendar_csv('CalendarDates', 'calendar_df.csv')\n",
    "\n",
    "adt_grouped = adt_df.copy()\n",
    "# Work with only Startpoint records in arrival departure times, will merge with Endpoint records later\n",
    "adt_grouped = adt_df.loc[adt_df.point_type == 'Startpoint'].groupby(['route_id', 'direction_id', 'actual'], observed=True)\n",
    "schedule_grouped = schedule.loc[schedule.stop_sequence == 1].groupby(['route_id', 'direction_id', 'scheduled'], observed=True)\n",
    "\n",
    "# Create two lists to store the unmatched names and groups\n",
    "unmatched_names = []\n",
    "unmatched_groups = []\n",
    "\n",
    "for name, group in adt_grouped:\n",
    "    # print the group if an error occurs\n",
    "    if name in schedule_grouped.groups:\n",
    "        # extract the corresponding group from schedule_route10_grouped\n",
    "        schedule_group = schedule_grouped.get_group(name)\n",
    "        schedule_services = set(schedule_group['service_id'])\n",
    "        schedule_service_block_ids = schedule_group.groupby(['service_id'])['block_id'].apply(list)\n",
    "\n",
    "        # extract the subset of the calendar_df that matches the service_date\n",
    "        service_days = calendar_df.loc[calendar_df.date.isin(group.service_date)]\n",
    "        # loop through the service_days\n",
    "        for i, row in service_days.iterrows():\n",
    "            # add the intersection between schedule_services and row['service_ids'] to the service_id column in adt_route10 as plain strings\n",
    "            adt_service_ids = schedule_services.intersection(row['service_ids'])\n",
    "            adt_service_ids_str = ', '.join(adt_service_ids)  # Convert set to string\n",
    "            adt_df.loc[group.index, 'service_id'] = adt_service_ids_str\n",
    "            # Get the block_id list associated to adt_service_ids_str\n",
    "            block_list=schedule_service_block_ids[schedule_service_block_ids.index==adt_service_ids_str]\n",
    "            block_list = block_list.iloc[0] if not block_list.empty else ''\n",
    "\n",
    "            if block_list and not group.empty:\n",
    "                # Concatenate block_list elements into a comma-separated string\n",
    "                block_ids_str = ', '.join(block_list)\n",
    "                # Assign the concatenated string to the specified rows in the column\n",
    "                adt_df.loc[group.index, 'block_id'] = block_ids_str\n",
    "    else:\n",
    "        unmatched_names.append(name)\n",
    "        unmatched_groups.append(group)\n",
    "\n",
    "print('Split Blocks')\n",
    "adt_df = split_multiple_block_id(adt_df)\n",
    "print('Assign Blocks to endpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the same values of block_id and service_id to Endpoint records with the same half_trip_id\n",
    "trips_grouped = adt_df.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    if len(group.index) > 1:\n",
    "        adt_df.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values\n",
    "    else:\n",
    "        print(name)\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_grouped = adt_df.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    if len(group.index) > 1:\n",
    "        adt_df.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values\n",
    "    else:\n",
    "        print(name)\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the same values of block_id and service_id to Endpoint records with the same half_trip_id\n",
    "trips_grouped = adt_df.groupby(['half_trip_id'], observed=True)\n",
    "for name, group in trips_grouped:\n",
    "    adt_df.loc[group.index[1], ['block_id', 'service_id']] = group.loc[group.index[0], ['block_id', 'service_id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute layover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layover_df as a copy\n",
    "layover_df = adt_route10.copy()\n",
    "layover_df = layover_df.sort_values(by=['block_id','service_date','half_trip_id','departure_time'])\n",
    "layover_df=layover_df.reset_index(drop=True)\n",
    "\n",
    "# Group by 'block_id' and 'service_date', skipping rows with null 'service_id' or 'block_id'\n",
    "grouped = layover_df.loc[layover_df.block_id.notna()].groupby(['block_id', 'service_date'])\n",
    "\n",
    "# Calculate theoretical and actual layover times using diff()\n",
    "layover_df['theoretical_layover'] = grouped['departure_time'].diff().dt.total_seconds() / 60\n",
    "layover_df['actual_layover'] = grouped['actual'].diff().dt.total_seconds() / 60\n",
    "\n",
    "# Replace the first row of each group with null timedelta\n",
    "layover_df.loc[grouped.head(1).index, ['theoretical_layover', 'actual_layover']] = 0\n",
    "layover_df.loc[layover_df.time_point_order != 1, ['theoretical_layover', 'actual_layover']] = np.nan\n",
    "layover_df = layover_df.drop(columns=['time_point_order', 'point_type', 'standard_type'])\n",
    "(layover_df.groupby(['block_id', 'service_date'])[['theoretical_layover', 'actual_layover']]\n",
    "            .agg({'theoretical_layover': ['mean', 'max', 'count'],\n",
    "                  'actual_layover': ['mean', 'max', 'count'],\n",
    "                  'stop_id': 'first'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = adt_route10.groupby(['half_trip_id'])\n",
    "startpoint_df = test.get_group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
